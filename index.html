<!DOCTYPE HTML>
<!--
	Retrospect by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->

<html>
	<head>

		<!-- realfavicongenerator.net -->
		<link rel="apple-touch-icon" sizes="180x180" href="images/favicons/white/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="images/favicons/white/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="images/favicons/white/favicon-16x16.png">
		<link rel="manifest" href="images/favicons/white/site.webmanifest">
		<link rel="mask-icon" href="images/favicons/white/safari-pinned-tab.svg" color="#0f1524">
		<link rel="shortcut icon" href="images/favicons/white/favicon.ico">
		<meta name="apple-mobile-web-app-title" content="Visuotactile Perception and Control 2020">
		<meta name="application-name" content="VisuoTactile 2020">
		<meta name="msapplication-TileColor" content="#da532c">
		<meta name="msapplication-config" content="images/favicons/white/browserconfig.xml">
		<meta name="theme-color" content="#ffffff">



		<title>Visuotactile Sensors for Robust Manipulation: From Perception to Control - RSS 2020</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<!-- <link rel="stylesheet" href="assets/css/main_orig.css" /> -->
		<link rel="stylesheet" href="assets/css/main_orig.css" />
		<link rel="stylesheet" href="assets/css/override.css" />
		<link rel="stylesheet" href="assets/css/photos.css">
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->

		<!-- for facebook -->
		<meta property="og:url"                content="http://www.alexalspach.com/workshops/visuotactile2020" />
		<meta property="og:type"               content="article" />
		<meta property="og:title"              content="Visuotactile Sensors for Robust Manipulation: From Perception to Control - RSS 2020" />
		
		<meta property="og:description"        content="This workshop will focus on a new class of tactile sensor in which visual sensor is used to “see touch” between a robot and its environment."/>
		
		<!--<meta property="og:image"              content="images/facebook/banner.jpg" />-->

	</head>



	<body class="landing">

		<!-- Header -->
			<header id="header" class="alt">
				<!--<div class="bighero-font"><a href="index.html">Humanoids2017</a></div>-->
				<h1><a href="#top">RSS 2020</a></h1>
				<!--<a href="index.html" class="position: absolute; display: inline-block; left: 1.25em; margin: 0; padding: 0;">Humanoids2017</a> -->
				<a href="#nav">Menu</a>
			</header>

		<!-- Nav -->
			<nav id="nav">
				<ul class="links">
					<li><a href="#top">Top</a></li>
					<!-- <li><a href="#call">Call for Contributions</a></li>-->
					<li><a href="#objectives">Objectives</a></li> 
					<li><a href="#schedule">Schedule</a></li>
					<li><a href="#speakers">Speakers</a></li>
					<li><a href="#organizers">Organizers</a></li>
					<li><a href="#links">Related Links</a></li>
					<li><a href="#contact">Contact</a></li>
					<!--<li><a href="photos.html">Photos</a></li>-->
					
				</ul>
			</nav>

		<!-- Banner -->
		<span class="anchor" id="top"></span>
			<section id="banner">
				<!--<i class="icon fa-diamond"></i>-->
					
				<br>

				<h2><strong>Visuotactile Sensors for Robust Manipulation:
					<br>From Perception to Control</strong></h2>	
				<h3><strong>(Proposed RSS Workshop)</strong></h3>

				<br>
				<p>Corvallis, Oregon
				<br>July 12 or 13, 2020</p> 
				
				<p><strong>Room TBD</strong></p> 

				
				<!--
				<br>
				<ul class="actions">
					<li><a href="photos.html" class="button big special bighero-font">Photos</a></li>
				</ul>
				-->
				


				<!--
				<div id="baymax"><img style="height: 100%;" src="images/baymax/baymax.png" /></div>
				-->


			</section>





		<div id="redbar"></div>
		<!--<span class="anchor" id="call"></span>-->
		<span class="anchor" id="objectives"></span>
		<section id="one" class="wrapper style2">

			<div class="inner">
				<h2><strong>Objectives</strong></h2>
				<!--<h2><strong>Call For Contributions</strong></h2>-->
				<p>The workshop will focus on a new class of sensors where a visual sensor (camera, depth camera, etc.) is used in order to perceive the contact state between a robot and the environment (i.e. to “see touch”). Considering the high-resolution pixel-based tactile information provided by these sensors, we wish to explore how they can be used towards robust manipulation via both model-based and learning-based approaches, consider what kinds of physical measurements can be performed or inferred from visuotactile information, and discuss what potential improvements should be made to current sensors and techniques associated with them.</p>


				<p>We invite you to contribute and to participate in this workshop.</p>

				<br>
 
				<strong>The workshop's topics include, but are not limited to:</strong>
				<ul>
					<li>...</li>
				</ul>

				<br>

				<strong>Confirmed Speakers:</strong>
				<ul>

					<li><a href="#anchor_adelson">Ted Adelson</a>, Massachusetts Institute of Technology, USA</li> 

					<li><a href="#anchor_atkeson">Chris Atkeson</a>, Carnegie Mellon University, USA</li>

					<li><a href="#anchor_bannerjee">Tapomayukh Bannerjee</a>, University of Washington, USA</li>

					<li><a href="#anchor_calandra">Roberto Calandra</a>, Facebook, USA</li>

					<li><a href="#anchor_haschke">Robert Haschke</a>, Bielefeld University, Germany</li>

					<li><a href="#anchor_kuppuswamy">Naveen Kuppuswamy</a>, Toyota Research Institute, USA</li>

					<li><a href="#anchor_rodriquez">Alberto Rodriquez</a>, Massachusetts Institute of Technology, USA</li>

					<li><a href="#anchor_shimonomura">Kazuhiro Shimonomura</a>, Ritsumeikan Institute of Technology, Japan</li>

					<li><a href="#anchor_yuan">Wenzhen Yuan</a>, Carnegie Mellon University, USA</li>

				</ul>

				<!--
				<strong>Tentative Speakers:</strong>
				<ul>
					<li><a href="#anchor_alspach">Alex Alspach</a>, Toyota Research Institute, USA</li>
						
				</ul>
				-->		

				<!--
				<br>		

				<strong>Previous Workshops:</strong>
				<ul>

					<li>2018: <a target="_blank" href="http://www.cs.cmu.edu/~cga/humanoids18workshop/ ">Fail-Safe HW & SW and Learning in Humanoid Robots</a></li>
					
					<li>2017: <a target="_blank" href="http://www.cs.cmu.edu/~cga/humanoids17workshop/ ">Design and Control for Soft Human-Robot Interaction</a></li>

					<li>2016: <a target="_blank" href="http://www.cs.cmu.edu/~cga/humanoids16workshop/ ">Making Hard Robots Soft: Sensors, Skin and Airbags</a></li>
					
					<li>2015: <a target="_blank" href="http://www.cs.cmu.edu/~cga/humanoids15workshop/">Soft Robotics and Safe Human-Robot Interaction in Humanoids</a></li>
					
				</ul>
				-->

				
				<!--

				<br>
				<h2><strong>Contributions:</strong></h2>

				<p>The workshop will gladly host contributions including short talks, posters, or any others proposed.</p>
				<br>



				<strong>Important dates:</strong>
				<ul>	
					
					<li>Sept. X, 2019: Submissions due</li>

					<li>Sept. X, 2019: Acceptance notification</li>

					<li>Sept. X, 2019: Short Talk Session Notification</li>
					
					
					
					<li><font color="red">Oct. XX, 2019:</font> Short Talk Session Deadline Extended!
					
					
					<li>Oct. 15, 2019: Can we build Baymax? Workshop</li>
				</ul>


				<br>
				<h2><strong>Abstract Submission</strong></h2>

				<p>The workshop will host a session of short talks. The presenters in this session will give 15 minute talks.</p>
 
				<p>To participate, please submit an abstract (1-2 pages, double-column, PDF) via email to <br>
					Joohyung Kim ( joohyung.kim at disney dot com ) or <br>
					Jinoh Lee ( jinoh.lee at iit dot it )</p>


				<p>Submission templates: 
				<a target="_blank" href="http://ras.papercept.net/conferences/support/tex.php">LaTeX</a>, <a target="_blank" href="http://ras.papercept.net/conferences/support/word.php">MS Word</a></p>

				-->
				
			</div>
		</section>



		<div id="redbar"></div>




		<!-- Two -->
		<span class="anchor" id="schedule"></span>
		<section id="two" class="wrapper style1">
			<div class="inner">

				<h2><strong>Workshop Schedule</strong></h2>


				<table class="schedule" width="100%" border="0" cellspacing="0" cellpadding="0">
					<tbody>

						<tr>
							<td class="sched_time"><strong>TBD</strong></td>
							
						</tr>
						
					</tbody>
					
				</table>


				<!--
				<table class="schedule" width="100%" border="0" cellspacing="0" cellpadding="0">
					<tbody>

						<tr>
							<td class="sched_time">08:50 - 09:00</td>
							<td class="sched_speaker">Welcome and Introduction</td>
						</tr>

						<tr>
							<td class="sched_time">09:00 - 09:30</td>
							<td class="sched_speaker">Katsu Yamane, Honda Research Institute, USA
							<div class="talk-title">Empathetic Physical Interaction</div>


							  	<div class="text-content short-text">
							  	<br>
	        					<p>Physical interactions using the body play an important role in human-human interactions by allowing the exchange of subtle information that is difficult to describe in words, e.g. comfort, preference, intention, and emotion. Robots working closely with humans have to understand, utilize and express such information in order to effectively interact with humans. The goal of empathetic physical interaction is to realize human-centered physical support where the robot uses its body as a medium for exchanging subtle information with the human partner, and adapts its behavior based on the information to improve the human perception and, ultimately, overall performance of physical support. This talk will introduce two related ongoing projects at Honda Research Institute USA: perception of pedestrian avoidance behavior of a mobile robot, and modeling of intimate social interactions of a humanoid robot.</p>
	    						</div>
							    <div class="show-more">
							        <a href="#">Abstract</a>
							    </div>

							</td>
						</tr>

						<tr>
							<td class="sched_time">09:30 - 10:00</td>
							<td class="sched_speaker">David Hanson, Hanson Robotics, Hong Kong
								<div class="talk-title">Art meets Artificial Intelligence, or Why Humanizing Robots Can Be Useful and Cool</div>
		
				
							  	<div class="text-content short-text">
							  	<br>
	        					<p>Most robots and AI today are designed to be non-humanlike, and certainly for many uses they don't need to be humanoid. However, for key applications such as the arts, intuitive social interfaces for AI agents, certain kinds of therapy, and for scientific investigation of human behavior, humanlike robots can be very useful. This presentation covers the tech, arts, and history of Hanson robots, including transdisciplinary collaboration among artists, robotics & AI engineers, manufacturing, business development, and social sciences. By diversifying the creative landscape of robotics, bringing domains of engineering together with narrative and figurative arts, philosophy, ethics, biosciences, and AI, we hope to achieve a better toolkit to discover what's best in humanity, and to empower intelligent machines to work with people better. Investigating new forms of robots as works of art may also challenge our preconceptions and allow for surprise and wonder. Furthermore, making AI embodied and humanlike may facilitate a path of co-evolution, leading towards machines who may someday grow to become true friends to humanity, as trusted allies rather than mere slaves. Maybe, humanizing our machines will realize a more stable world with growing benefits for all sentient beings.</p>
	    						</div>
							    <div class="show-more">
							        <a href="#">Abstract</a>
							    </div>


							    

							</td>
						</tr>

						<tr>
							<td class="sched_time">10:00 - 10:30</td>
							<td class="sched_speaker">Gordon Cheng, Technical University of Munich, Germany
								<div class="talk-title">TBD</div>
							</td>
						</tr>

						<tr>
							<td class="sched_time">10:30 - 11:00</td>
							<td class="sched_speaker">Coffee Break (Lobby)
							</td>
						</tr>						

						<tr>
							<td class="sched_time">11:00 - 11:30</td>
							<td class="sched_speaker">Serena Ivaldi, INRIA Nancy Grand-Est, France
								<div class="talk-title">Social and physical interaction with a humanoid: Lessons learned with the iCub</div>	

							  	<div class="text-content short-text">
							  	<br>
	        					<p>In this talk I will overview our recent human-humanoid interaction experiments with the iCub humanoid and discuss our findings. We studied social and physical interaction in a collaborative assembly scenario to study engagement and relation to individual factors. We studied how to use these signals for predicting intention in collaborative scenarios, which led us to a multimodal intention prediction framework based on probabilistic movement primitives. Finally, we studied trust in human-humanoid interaction with a shared decision protocol, which unveiled the people’s negative trusting attitude towards the robot. I will conclude with our current perspectives in whole-body collaboration.</p>
	    						</div>
							    <div class="show-more">
							        <a href="#">Abstract</a>
							    </div>

							</td>
						</tr>

						<tr>
							<td class="sched_time">11:30 - 12:00</td>
							<td class="sched_speaker">Chung Hyuk Park, George Washington University, USA
								<div class="talk-title">Empathetic Agent and Interactions with Children with ASD</div>

							  	<div class="text-content short-text">
							  	<br>
	        					<p>Socially assistive robots (SARs) and their application for interventions for children with autism spectrum disorder (ASD) have been actively researched and widely used in special education and clinical settings. Going further from their utilization as learning aids and research platforms, this talk will address the aspects of empathy and emotion regulation (ER) for SARs, which are important mechanisms to be implemented in interventions since the empathy and ER impairments are underlying factors for many atypicalities manifested in ASD. We will discuss the design of our empathetic robotic agent, including the design and the algorithmic model to provide dynamic ER capabilities. In addition, we will describe a user study that evaluates the ER capabilities of an emotionally expressive empathetic agent as well as its capability to prime higher social engagement in a user.</p>
	    						</div>
							    <div class="show-more">
							        <a href="#">Abstract</a>
							    </div>


							</td>
						</tr>


						<tr>
							<td class="sched_time">12:00 - 12:30</td>
							<td class="sched_speaker">Hae Won Park, MIT Media Lab, USA
								<div class="talk-title">Socio-Emotive Intelligence for Long-term Human-Robot Interaction</div>

							  	<div class="text-content short-text">
							  	<br>
	        					<p>In this talk, I’d like to engage our community to question whether robots need socio-emotive intelligence. To answer this question though, we need to first think about a new dimension of evaluating AI algorithms and systems that we build - measuring their impact on people’s lives in the real-world contexts. I will highlight a number of provocative research findings from our recent long-term deployment of social robots in schools, homes, and older adult living communities. We employ an affective reinforcement learning approach to personalize robot’s actions to modulate each user’s engagement and maximize the interaction benefit. The robot observes users’ verbal and nonverbal affective cues to understand the user state and to receive feedback on its actions. Our results show that the interaction with a robot companion influences users’ beliefs, learning, and how they interact with others. The affective personalization boosts these effects and helps sustain long-term engagement. During our deployment studies, we observed that people treat and interact with artificial agents as social partners and catalysts. We also learned that the effect of the interaction strongly correlates to the social relational bonding the user has built with the robot. So, to answer the question “does a robot need socio-emotive intelligence,” I argue that we should only draw conclusions based on what impact it has on the people living with it -  is it helping us flourish in the direction that we want to thrive?</p>
	    						</div>
							    <div class="show-more">
							        <a href="#">Abstract</a>
							    </div>


							</td>
						</tr>



						<tr>
							<td class="sched_time">12:30 - 14:00</td>
							<td class="sched_speaker">Lunch
							</td>
						</tr>


						<tr>
							<td class="sched_time">14:00 - 14:30</td>
							<td class="sched_speaker">Ludovic Righetti, New York University, USA & Max-Planck Institute, Germany
								<div class="talk-title">What can go wrong when robots physically interact with humans? An ethical and technical perspective</div>

							  	<div class="text-content short-text">
							  	<br>
	        					<p>The ability to safely physically interact with humans is one of the most exciting features of Baymax. This presentation will tackle two drastically different, yet, inseparable issues when discussing robots that interact with humans: the technical problem of safe physical interaction and the ethical issues inherent to robots making decisions that impact us. The first part of the presentation will present our research related to the control of contact interactions, using both optimal control and reinforcement learning and highlight some of the current challenges one faces when creating safe physical interactions. In the second part of the talk, we will discuss some issues that can arise when robots interact with humans and make decisions that have an impact on how humans receive a service or health care support. We will give an overview of concerns associated to bias in machine learning that can lead to discriminatory behaviors and explain how this impacts robotics. Then we will give some possible research directions on how to tackle these issues to create safe robot companions that can benefit everyone.</p>
	    						</div>
							    <div class="show-more">
							        <a href="#">Abstract</a>
							    </div>


							</td>
						</tr>

						<tr>
							<td class="sched_time">14:30 - 15:00</td>
							<td class="sched_speaker">Alex Alspach, Toyota Research Institute, USA
								<div class="talk-title">Tactile sensing bubbles for interaction</div>

							</td>
						</tr>	


						<tr>
							<td class="sched_time">15:00 - 15:30</td>
							<td class="sched_speaker">Joohyung Kim, Disney Research, USA
								<div class="talk-title">Robots inspired by animation characters</div>


							  	<div class="text-content short-text">
							  	<br>
	        					<p>Animated characters often have interesting and unique motions, configurations and abilities. Animators mostly base these creatures on nature, including humans, and augment them with their imagination. As technology advances, some of these features become implementable in real robots. In this talk, I will present our efforts at Disney Research to make robots that capture these interesting features from animation characters.</p>
	    						</div>
							    <div class="show-more">
							        <a href="#">Abstract</a>
							    </div>


							</td>
						</tr>	


						<tr>
							<td class="sched_time">15:30 - 16:00</td>
							<td class="sched_speaker">Coffee Break (Lobby)
							</td>
						</tr>	



						<tr>
							<td class="sched_time">16:00 - 16:30</td>
							<td class="sched_speaker">Communicate with Speakers</td>
						</tr>

						<tr>
							<td class="sched_time">16:30 - 17:30</td>
							<td class="sched_speaker">Open discussion and Closing</td>
						</tr>

						<tr>
							<td class="sched_time">18:00 - </td>
							<td class="sched_speaker">Welcome Reception and Late Breaking Reports</td>
						</tr>
						
					</tbody>
					
				</table>

				-->
				



			</div>
		</section>		

		<!-- Three -->
		<span class="anchor" id="speakers"></span>
		<section id="three" class="wrapper style2">
			<div class="inner">

				<h2><strong>Speakers</strong></h2>


				<ul>
					<li><a href="#anchor_adelson">Ted Adelson</a>, Massachusetts Institute of Technology, USA</li> 
					<li><a href="#anchor_atkeson">Chris Atkeson</a>, Carnegie Mellon University, USA</li>
					<li><a href="#anchor_bannerjee">Tapomayukh Bannerjee</a>, University of Washington, USA</li>
					<li><a href="#anchor_calandra">Roberto Calandra</a>, Facebook, USA</li>
					<li><a href="#anchor_haschke">Robert Haschke</a>, Bielefeld University, Germany</li>
					<li><a href="#anchor_kuppuswamy">Naveen Kuppuswamy</a>, Toyota Research Institute, USA</li>
					<li><a href="#anchor_rodriquez">Alberto Rodriquez</a>, Massachusetts Institute of Technology, USA</li>
					<li><a href="#anchor_shimonomura">Kazuhiro Shimonomura</a>, Ritsumeikan Institute of Technology, Japan</li>
					<li><a href="#anchor_yuan">Wenzhen Yuan</a>, Carnegie Mellon University, USA</li>
				</ul>

				<br>


				<span class="anchor2" id="anchor_adelson"></span>
				<article class="feature left">
					<span class="image"><img src="images/speakers/web/placeholder.jpg" alt="" /></span>
					<div class="content">

						<h2>Ted Adelson</h2>

						<p>Ted Adelson is well known for contributions to multiscale image representation (such as the Laplacian pyramid) and basic concepts in early vision such as motion energy and steerable filters (honored by the IEEE Computer Society’s Helmholtz Prize, 2013). His work on the neural mechanisms of motion perception was honored with the Rank Prize in Optoelectronics (1992). His work on layered representations for motion won the IEEE Computer Society’s Longuet-Higgins Award (2005). He introduced the plenoptic function, and built the first plenoptic camera. He has done pioneering work on the problems of material perception in human and machine vision. He has produced some well known illusions such as the Checker-Shadow Illusion. Prof. Adelson has recently developed a novel technology for artificial touch sensing, called GelSight, which converts touch to images, and which enables robots to have tactile sensitivity exceeding that of human skin</p>

					</div>
				</article>


				<span class="anchor2" id="anchor_atkeson"></span>
				<article class="feature left">
					<span class="image object-fit_contain"><img src="images/speakers/web/Atkeson_Chris.jpg" alt="" /></span>
					<div class="content">

						<h2>Christopher G. Atkeson</h2>

						<p>I am a Professor in the Robotics Institute and Human-Computer Interaction Institute at Carnegie Mellon University. My life goal is to fulfill the science fiction vision of machines that achieve human levels of competence in perceiving, thinking, and acting. A more narrow technical goal is to understand how to get machines to generate and perceive human behavior. I use two complementary approaches, exploring humanoid robotics and human aware environments. Building humanoid robots tests our understanding of how to generate human-like behavior, and exposes the gaps and failures in current approaches.</p>

						<p><a target="_blank" href="http://www.build-baymax.org">build-baymax.org</a></p>

					</div>
				</article>


				<span class="anchor2" id="anchor_bannerjee"></span>
				<article class="feature left">
					<span class="image object-fit_contain"><img src="images/speakers/web/placeholder.jpg" alt="" /></span>
					<div class="content">

						<h2>Tapomayukh Bannerjee</h2>

						<p>BIO COMING SOON</p>

					</div>
				</article>		


				<span class="anchor2" id="anchor_calandra"></span>
				<article class="feature left">
					<span class="image object-fit_contain"><img src="images/speakers/web/placeholder.jpg" alt="" /></span>
					<div class="content">

						<h2>Roberto Calandra</h2>

						<p>Roberto Calandra is a Research Scientist at Facebook AI Research. Previously, he was a Postdoctoral Scholar at the University of California, Berkeley (US) in the Berkeley Artificial Intelligence Research Laboratory (BAIR) working with Sergey Levine. His education includes a Ph.D. from TU Darmstadt (Germany) under the supervision of Jan Peters and Marc Deisenroth, a M.Sc. in Machine Learning and Data Mining from the Aalto university (Finland), and a B.Sc. in Computer Science from the Università degli studi di Palermo (Italy).</p>

					</div>
				</article>	


				<span class="anchor2" id="anchor_haschke"></span>
				<article class="feature left">
					<span class="image object-fit_contain"><img src="images/speakers/web/placeholder.jpg" alt="" /></span>
					<div class="content">

						<h2>Robert Haschke</h2>

						<p>Robert Haschke received the diploma and PhD in Computer Science from the University of Bielefeld, Germany, in 1999 and 2004, working on the theoretical analysis of oscillating recurrent neural networks. Since then, his work focuses more on robotics, still employing neural methods whereever possible. Robert is currently heading the Robotics Group within the Neuroinformatics Group, striving to enrich the dexterous manipulation skills of our two bimanual robot setups through interactive learning. His fields of research include neural networks, cognitive bimanual robotics, grasping and manipulation with multi-fingered dexterous hands, tactile sensing, and software integration.</p>

					</div>
				</article>	


				<span class="anchor2" id="anchor_rodriquez"></span>
				<article class="feature left">
					<span class="image object-fit_contain"><img src="images/speakers/web/placeholder.jpg" alt="" /></span>
					<div class="content">

						<h2>Alberto Rodriquez</h2>

						<p>Alberto Rodriguez is an Associate Professor (without tenure) at the Mechanical Engineering Department at MIT. Alberto graduated in Mathematics ('05) and Telecommunication Engineering ('06) from the Universitat Politecnica de Catalunya, and earned his PhD (’13) from the Robotics Institute at Carnegie Mellon University. He leads the Manipulation and Mechanisms Lab at MIT (MCube) researching autonomous dexterous manipulation, robot automation, and end-effector design. Alberto has received Best Paper Awards at conferences RSS’11, ICRA’13, RSS’18, IROS'18, and RSS'19, and has been finalist for best paper awards at conferences IROS’16 and IROS'18. He has lead Team MIT-Princeton in the Amazon Robotics Challenge between 2015 and 2017, and has received the Amazon Research Award in 2018 and 2019, and the 2018 Best Manipulation System Paper Award from Amazon. </p>

					</div>
				</article>	


				<span class="anchor2" id="anchor_shimonomura"></span>
				<article class="feature left">
					<span class="image object-fit_contain"><img src="images/speakers/web/placeholder.jpg" alt="" /></span>
					<div class="content">

						<h2>Kazuhiro Shimonomura</h2>

						<p>Kazuhiro Shimonomura is a Professor in Department of Robotics, Ritsumeikan University, Shiga, Japan. His current research interests include "Aerial robotics", "Tactile image sensing", and "Ultra-high speed imaging".</p>

					</div>
				</article>	


			</div>
		</section>
		




		<!-- Four -->
		<span class="anchor" id="organizers"></span>
		<section id="four" class="wrapper style1">
			<div class="inner">

				<h2><strong>Organizers</strong></h2>

				
				<ul>
					<li><a href="#anchor_alspach">Alex Alspach</a>, Toyota Research Institute</li>
					<li><a href="#anchor_kuppuswamy">Naveen Kuppuswamy</a>, Toyota Research Institute, USA</li>
					<li><a href="#anchor_uttamchandani">Avinash Uttamchandani</a>, Toyota Research Institute</li>
					<li><a href="#anchor_veiga">Filipe Veiga</a>, Massachusetts Institute of Technology, USA</li>
					<li><a href="#anchor_yuan">Wenzhen Yuan</a>, Carnegie Mellon University, USA</li>
				</ul>



				<br>

				<span class="anchor2" id="anchor_alspach"></span>
				<article class="feature left">
					<span class="image object-fit_contain"><img src="images/speakers/web/Alspach_Alex.jpg" alt="" /></span>
					<div class="content">

						<h2>Alex Alspach</h2>

						<p>Alex designs and builds soft systems for sensing and manipulation at Toyota Research Institute (TRI). He earned his master's degree at Drexel University with time spent in the Drexel Autonomous Systems Lab (DASL) and KAIST's HuboLab. After graduating, Alex spent two years at SimLab in Korea developing and marketing tools for manipulation research. While there, he also worked with a production company to develop artists' tools for animating complex, synchronized industrial robot motions. Prior to joining TRI, Alex developed soft <i>huggable</i> robots and various other systems at Disney Research with Joohyung and Katsu!</p>

					</div>
				</article>	


				<span class="anchor2" id="anchor_kuppuswamy"></span>
				<article class="feature left">
					<span class="image object-fit_contain"><img src="images/speakers/web/placeholder.jpg" alt="" /></span>
					<div class="content">

						<h2>Naveen Kuppuswamy</h2>

						<p>Naveen Kuppuswamy is a senior research scientist at the Toyota Research Institute (TRI). His current research interests are on tactile perception and control for manipulation and soft robotics.</p>

					</div>
				</article>	


				<span class="anchor2" id="anchor_uttamchandani"></span>
				<article class="feature left">
					<span class="image object-fit_contain"><img src="images/speakers/web/placeholder.jpg" alt="" /></span>
					<div class="content">

						<h2>Avinash Uttamchandani</h2>

						<p>Avinash Uttamchandani is an electrical engineer working on manipulation research at the Toyota Research Institute, focusing on tactile sensing, embedded electronics, and real-time signal processing and controls.<p>

					</div>
				</article>					


				<span class="anchor2" id="anchor_veiga"></span>
				<article class="feature left">
					<span class="image object-fit_contain"><img src="images/speakers/web/placeholder.jpg" alt="" /></span>
					<div class="content">

						<h2>Filipe Veiga</h2>

						<p>Filipe Veiga is a Postdoctoral Associate at the Computer Science & Artificial Intelligence Lab at the Massachusetts Institute of Technology. His research focuses on exploring how the sense of touch can be used to improve the dexterous manipulation skills of robots.<p>

					</div>
				</article>	

				<span class="anchor2" id="anchor_yuan"></span>
				<article class="feature left">
					<span class="image object-fit_contain"><img src="images/speakers/web/Yuan_Wenzhen.jpg" alt="" /></span>
					<div class="content">

						<h2>Wenzhen Yuan</h2>

						<p>Wenzhen Yuan is an assistant professor at the Robotics Institute, Carnegie Mellon University. Her research is on developing high-resolution tactile sensors, and applying them for robot manipulation and perception.</p>

					</div>
				</article>	


				<!-- Logos -->
				<!--
				<div class="logo_container2">
					<center>
					<img class="org_logos2 object-fit_contain" src="images/logos/web/logos5.png" alt="" />
					</center>
				</div>
				-->

				
			</div>
		</section>

		<!-- Five -->
		<span class="anchor" id="links"></span>
		<section id="five" class="wrapper style2">
			<div class="inner">

				<h2><strong>Related Links</strong></h2>

				
				<ul>
					
					<li><a target="_blank" href="https://roboticsconference.org/">RSS 2020</a></li>

				</ul>
				
			</div>
		</section>

		<!-- Five -->
		<span class="anchor" id="contact"></span>
		<section id="five" class="wrapper style1">
			<div class="inner">

				<h2><strong>Contact</strong></h2>

				
				<ul>
					<li>Avinash Uttamchandani ( avinash.uttamchandani at tri dot global )</li>
				</ul>
				
			</div>
		</section>		






		<!-- Footer -->
		<footer id="footer">
			<div class="inner">
				<p><a target="_blank" href="http://alexalspach.com">Designed by Alex Alspach. 2020</a></p>
				
			</div>
		</footer>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/skel.min.js"></script>
		<script src="assets/js/util.js"></script>
		<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
		<script src="assets/js/main.js"></script>


		<script>
			$(".show-more a").each(function() {
			    var $link = $(this);
			    var $content = $link.parent().prev("div.text-content");

			    console.log($link);

			    var visibleHeight = $content[0].clientHeight;
			    var actualHide = $content[0].scrollHeight - 1;

			    console.log(actualHide);
			    console.log(visibleHeight);

			    if (actualHide > visibleHeight) {
			        $link.show();
			    } else {
			        $link.hide();
			    }
			});

			$(".show-more a").on("click", function() {
			    var $link = $(this);
			    var $content = $link.parent().prev("div.text-content");
			    var linkText = $link.text();

			    $content.toggleClass("short-text, full-text");

			    $link.text(getShowLinkText(linkText));

			    return false;
			});

			function getShowLinkText(currentText) {
			    var newText = '';

			    if (currentText.toUpperCase() === "ABSTRACT") {
			        newText = "Hide Abstract";
			    } else {
			        newText = "Abstract";
			    }

			    return newText;
			}
		</script>




	</body>
</html>




